import numpy as np

class MLAlgorithmTemplate:
    def __init__(self, hyperparameters=None):
        #  when parameters are assigned 
        self.parameters_assigned = False
       
        self.learned_parameters = None  #  learned parameters
        #  hyperparameters 
        self.hyperparameters = hyperparameters
    
    # Training function 
    def train(self, X, y):
        raise NotImplementedError("Subclasses must implement this function.  IMPLMENT ERROR!")
    
    
    def predict(self, X):
        raise NotImplementedError("Subclasses must implement this function.  IMPLMENT ERROR!")
    
   
    def get_parameters_assigned(self):
        return self.parameters_assigned
    
    def get_learned_parameters(self):
        return self.learned_parameters
    
    def get_hyperparameters(self):
        return self.hyperparameters
    
    # update  parameters 
    def _update_learned_parameters(self, parameters):
        self.learned_parameters = parameters
        self.parameters_assigned = True

class LogisticRegression(MLAlgorithmTemplate):
    def __init__(self, hyperparameters=None):
        super().__init__(hyperparameters)

    def train(self, X, y):
        # Ensure hyperparameters are provided
        if not self.hyperparameters:
            raise ValueError("Hyperparameters are needed, ERROR ! REQS!")
        
        # Extract hyperparameters
        learn_pace = self.hyperparameters.get("learn_pace", 0.01)  
        convergence_threshold = self.hyperparameters.get("convergence_threshold", 1e-5)
        max_iterations = self.hyperparameters.get("max_iterations", 1000)
        l2_penalty = self.hyperparameters.get("l2_penalty", 0.01)  
        clip_value = self.hyperparameters.get("clip_value", 1.0)  

        # Initialize parameters to random values
        num_features = X.shape[1] + 1  
        np.random.seed(42)
        self.learned_parameters = np.random.randn(num_features)
    
        intercept_column = np.ones((X.shape[0], 1))
        X_augmented = np.hstack((intercept_column, X))

        for iteration in range(max_iterations):
            # Backup current parameters (theta_prev)
            theta_prev = self.learned_parameters.copy()
            gradient = self._compute_gradient(X_augmented, y, theta_prev)
            gradient += l2_penalty * theta_prev
            gradient = np.clip(gradient, -clip_value, clip_value)
            self.learned_parameters -= learn_pace * gradient
            loss = self._compute_loss(X_augmented, y)

            # Print debugging information
            print(f"Iteration {iteration}:")
            print(f"Gradient: {gradient}")
            print(f"Parameters: {self.learned_parameters}")
            print(f"Loss: {loss}\n")

            # Check for convergence
            diff = self.learned_parameters - theta_prev
            l2_norm_squared = np.dot(diff, diff)  
            if l2_norm_squared <= convergence_threshold:
                print(f"Convergence reached at iteration {iteration}")
                break

        # Store the learned parameters
        self._update_learned_parameters(self.learned_parameters)
        print(f"Gradient descent completed after {iteration + 1} iterations")

    def predict(self, X):
        if not self.parameters_assigned:
            raise ValueError("Model parameters have not been learned yet. Please train the model first.")
        intercept_column = np.ones((X.shape[0], 1))
        X_augmented = np.hstack((intercept_column, X))

        # Compute predictions using the learned parameters
        z = X_augmented @ self.learned_parameters
        probabilities = 1 / (1 + np.exp(-z))

        
        predictions = (probabilities >= 0.5).astype(int)
        return predictions

    def _predict_probability(self, X):
        
        z = X @ self.learned_parameters
        probabilities = 1 / (1 + np.exp(-z))
        return probabilities

    def _compute_gradient(self, X, y, theta):
        # Compute the gradient of the log 
        probabilities = self._predict_probability(X)
        errors = probabilities - y
        gradient = X.T @ errors / len(y)  # Average acroos of the  all samples

        return gradient

    def _compute_loss(self, X, y):
        # Compute the logistic loss 
        z = X @ self.learned_parameters
        probabilities = 1 / (1 + np.exp(-z))
        loss = -np.mean(y * np.log(probabilities + 1e-15) + (1 - y) * np.log(1 - probabilities + 1e-15))
        return loss

# Example usage
if __name__ == "__main__":
   
    X_train = np.array([[1, 2], [2, 3], [3, 4]])
    y_train = np.array([0, 1, 0])

    
    hyperparams = {
        "learn_pace": 0.01,        
        "convergence_threshold": 1e-5,   
        "max_iterations": 1000,
        "l2_penalty": 0.01,              
        "clip_value": 1.0              
    }

    log_reg = LogisticRegression(hyperparameters=hyperparams)

    log_reg.train(X_train, y_train)

    # Predict class labels for the training data
    predictions = log_reg.predict(X_train)
    print("Predictions (class labels):", predictions)
